{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dropout Implementation \n\nWhat is dropout?\nDropout refers to data, or noise, that's intentionally dropped from a neural network to improve processing and time to results.\nThe human brain contains billions of neurons that fire electrical and chemical signals to each other to coordinate thoughts and life functions. A neural network uses a software equivalent of these neurons, called units. Each unit receives signals from other units and then computes an output that it passes onto other neuron/units, or nodes, in the network.\nGo through this link to know more about **Dropout**\n\nhttps://www.kaggle.com/discussions/general/397662","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:39:14.792138Z","iopub.execute_input":"2023-03-28T06:39:14.792810Z","iopub.status.idle":"2023-03-28T06:39:23.564291Z","shell.execute_reply.started":"2023-03-28T06:39:14.792772Z","shell.execute_reply":"2023-03-28T06:39:23.563082Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Load Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/sonar-datasets/sonar_dataset.csv\", header = None) # here I used header = None as this dataset doesn't have any header\ndf.sample(5)#it will show only 5 rows with all columns\n#df # it will show all the rows and colums","metadata":{"execution":{"iopub.status.busy":"2023-03-28T05:48:57.525912Z","iopub.execute_input":"2023-03-28T05:48:57.526719Z","iopub.status.idle":"2023-03-28T05:48:57.558237Z","shell.execute_reply.started":"2023-03-28T05:48:57.526679Z","shell.execute_reply":"2023-03-28T05:48:57.557134Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       8   \\\n11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n94   0.0025  0.0309  0.0171  0.0228  0.0434  0.1224  0.1947  0.1661  0.1368   \n125  0.0228  0.0853  0.1000  0.0428  0.1117  0.1651  0.1597  0.2116  0.3295   \n57   0.0216  0.0124  0.0174  0.0152  0.0608  0.1026  0.1139  0.0877  0.1160   \n79   0.0108  0.0086  0.0058  0.0460  0.0752  0.0887  0.1015  0.0494  0.0472   \n\n         9   ...      51      52      53      54      55      56      57  \\\n11   0.0835  ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n94   0.1430  ...  0.0149  0.0077  0.0036  0.0114  0.0085  0.0101  0.0016   \n125  0.3517  ...  0.0172  0.0191  0.0260  0.0140  0.0125  0.0116  0.0093   \n57   0.0866  ...  0.0052  0.0049  0.0096  0.0134  0.0122  0.0047  0.0018   \n79   0.0393  ...  0.0029  0.0078  0.0114  0.0083  0.0058  0.0003  0.0023   \n\n         58      59  60  \n11   0.0009  0.0044   R  \n94   0.0028  0.0014   R  \n125  0.0012  0.0036   M  \n57   0.0006  0.0023   R  \n79   0.0026  0.0027   R  \n\n[5 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>0.0123</td>\n      <td>0.0309</td>\n      <td>0.0169</td>\n      <td>0.0313</td>\n      <td>0.0358</td>\n      <td>0.0102</td>\n      <td>0.0182</td>\n      <td>0.0579</td>\n      <td>0.1122</td>\n      <td>0.0835</td>\n      <td>...</td>\n      <td>0.0133</td>\n      <td>0.0265</td>\n      <td>0.0224</td>\n      <td>0.0074</td>\n      <td>0.0118</td>\n      <td>0.0026</td>\n      <td>0.0092</td>\n      <td>0.0009</td>\n      <td>0.0044</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>0.0025</td>\n      <td>0.0309</td>\n      <td>0.0171</td>\n      <td>0.0228</td>\n      <td>0.0434</td>\n      <td>0.1224</td>\n      <td>0.1947</td>\n      <td>0.1661</td>\n      <td>0.1368</td>\n      <td>0.1430</td>\n      <td>...</td>\n      <td>0.0149</td>\n      <td>0.0077</td>\n      <td>0.0036</td>\n      <td>0.0114</td>\n      <td>0.0085</td>\n      <td>0.0101</td>\n      <td>0.0016</td>\n      <td>0.0028</td>\n      <td>0.0014</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>0.0228</td>\n      <td>0.0853</td>\n      <td>0.1000</td>\n      <td>0.0428</td>\n      <td>0.1117</td>\n      <td>0.1651</td>\n      <td>0.1597</td>\n      <td>0.2116</td>\n      <td>0.3295</td>\n      <td>0.3517</td>\n      <td>...</td>\n      <td>0.0172</td>\n      <td>0.0191</td>\n      <td>0.0260</td>\n      <td>0.0140</td>\n      <td>0.0125</td>\n      <td>0.0116</td>\n      <td>0.0093</td>\n      <td>0.0012</td>\n      <td>0.0036</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>0.0216</td>\n      <td>0.0124</td>\n      <td>0.0174</td>\n      <td>0.0152</td>\n      <td>0.0608</td>\n      <td>0.1026</td>\n      <td>0.1139</td>\n      <td>0.0877</td>\n      <td>0.1160</td>\n      <td>0.0866</td>\n      <td>...</td>\n      <td>0.0052</td>\n      <td>0.0049</td>\n      <td>0.0096</td>\n      <td>0.0134</td>\n      <td>0.0122</td>\n      <td>0.0047</td>\n      <td>0.0018</td>\n      <td>0.0006</td>\n      <td>0.0023</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>0.0108</td>\n      <td>0.0086</td>\n      <td>0.0058</td>\n      <td>0.0460</td>\n      <td>0.0752</td>\n      <td>0.0887</td>\n      <td>0.1015</td>\n      <td>0.0494</td>\n      <td>0.0472</td>\n      <td>0.0393</td>\n      <td>...</td>\n      <td>0.0029</td>\n      <td>0.0078</td>\n      <td>0.0114</td>\n      <td>0.0083</td>\n      <td>0.0058</td>\n      <td>0.0003</td>\n      <td>0.0023</td>\n      <td>0.0026</td>\n      <td>0.0027</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Data Exploration\n\nThe shape of a DataFrame is a tuple of array dimensions that tells the number of rows and columns of a given DataFrame.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-28T05:49:45.419093Z","iopub.execute_input":"2023-03-28T05:49:45.419492Z","iopub.status.idle":"2023-03-28T05:49:45.426736Z","shell.execute_reply.started":"2023-03-28T05:49:45.419457Z","shell.execute_reply":"2023-03-28T05:49:45.425574Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(208, 61)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now I want to know if there is any column that containing null or not","metadata":{}},{"cell_type":"markdown","source":"The isna() method returns a DataFrame object where all the values are replaced with a Boolean value True for NA (not-a -number) values, and otherwise False.\n","metadata":{}},{"cell_type":"code","source":"df.isna()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T05:54:21.862856Z","iopub.execute_input":"2023-03-28T05:54:21.863574Z","iopub.status.idle":"2023-03-28T05:54:21.893484Z","shell.execute_reply.started":"2023-03-28T05:54:21.863535Z","shell.execute_reply":"2023-03-28T05:54:21.892366Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"        0      1      2      3      4      5      6      7      8      9   \\\n0    False  False  False  False  False  False  False  False  False  False   \n1    False  False  False  False  False  False  False  False  False  False   \n2    False  False  False  False  False  False  False  False  False  False   \n3    False  False  False  False  False  False  False  False  False  False   \n4    False  False  False  False  False  False  False  False  False  False   \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n203  False  False  False  False  False  False  False  False  False  False   \n204  False  False  False  False  False  False  False  False  False  False   \n205  False  False  False  False  False  False  False  False  False  False   \n206  False  False  False  False  False  False  False  False  False  False   \n207  False  False  False  False  False  False  False  False  False  False   \n\n     ...     51     52     53     54     55     56     57     58     59     60  \n0    ...  False  False  False  False  False  False  False  False  False  False  \n1    ...  False  False  False  False  False  False  False  False  False  False  \n2    ...  False  False  False  False  False  False  False  False  False  False  \n3    ...  False  False  False  False  False  False  False  False  False  False  \n4    ...  False  False  False  False  False  False  False  False  False  False  \n..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n203  ...  False  False  False  False  False  False  False  False  False  False  \n204  ...  False  False  False  False  False  False  False  False  False  False  \n205  ...  False  False  False  False  False  False  False  False  False  False  \n206  ...  False  False  False  False  False  False  False  False  False  False  \n207  ...  False  False  False  False  False  False  False  False  False  False  \n\n[208 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 61 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The sum() method adds all values in each column and returns the sum for each column","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T05:54:33.191036Z","iopub.execute_input":"2023-03-28T05:54:33.192306Z","iopub.status.idle":"2023-03-28T05:54:33.217895Z","shell.execute_reply.started":"2023-03-28T05:54:33.192260Z","shell.execute_reply":"2023-03-28T05:54:33.216410Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0     0\n1     0\n2     0\n3     0\n4     0\n     ..\n56    0\n57    0\n58    0\n59    0\n60    0\nLength: 61, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"As there is no null values so I don't have to tackle the null values","metadata":{}},{"cell_type":"code","source":"df.columns #it just gives columns name. As we have no header& the columns name is only a neumerical values","metadata":{"execution":{"iopub.status.busy":"2023-03-28T05:58:31.447449Z","iopub.execute_input":"2023-03-28T05:58:31.448513Z","iopub.status.idle":"2023-03-28T05:58:31.456990Z","shell.execute_reply.started":"2023-03-28T05:58:31.448471Z","shell.execute_reply":"2023-03-28T05:58:31.455937Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n            51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n           dtype='int64')"},"metadata":{}}]},{"cell_type":"code","source":"df[60].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:14:19.557184Z","iopub.execute_input":"2023-03-28T06:14:19.557565Z","iopub.status.idle":"2023-03-28T06:14:19.571456Z","shell.execute_reply.started":"2023-03-28T06:14:19.557532Z","shell.execute_reply":"2023-03-28T06:14:19.570118Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"M    111\nR     97\nName: 60, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The drop() method removes the specified row or column. By specifying the column axis ( axis='columns' ), the drop() method removes the specified column.","metadata":{}},{"cell_type":"code","source":"x = df.drop(60, axis = 'columns')\ny = df[60]\nx","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:19:03.246429Z","iopub.execute_input":"2023-03-28T06:19:03.247114Z","iopub.status.idle":"2023-03-28T06:19:03.280552Z","shell.execute_reply.started":"2023-03-28T06:19:03.247075Z","shell.execute_reply":"2023-03-28T06:19:03.279402Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       8   \\\n0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n\n         9   ...      50      51      52      53      54      55      56  \\\n0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n\n         57      58      59  \n0    0.0084  0.0090  0.0032  \n1    0.0049  0.0052  0.0044  \n2    0.0164  0.0095  0.0078  \n3    0.0044  0.0040  0.0117  \n4    0.0048  0.0107  0.0094  \n..      ...     ...     ...  \n203  0.0115  0.0193  0.0157  \n204  0.0032  0.0062  0.0067  \n205  0.0138  0.0077  0.0031  \n206  0.0079  0.0036  0.0048  \n207  0.0036  0.0061  0.0115  \n\n[208 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0125</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0033</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0241</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0156</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0203</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0051</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0155</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0181</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Look above, now the dataset doesn't have 60 columns","metadata":{}},{"cell_type":"markdown","source":"Now we will see y. Here y is not ready because it has data in categorical form like R , M. So we need to do one hot coding to ocnvert it into integer","metadata":{}},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:19:08.106240Z","iopub.execute_input":"2023-03-28T06:19:08.106934Z","iopub.status.idle":"2023-03-28T06:19:08.115906Z","shell.execute_reply.started":"2023-03-28T06:19:08.106894Z","shell.execute_reply":"2023-03-28T06:19:08.114750Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0      R\n1      R\n2      R\n3      R\n4      R\n      ..\n203    M\n204    M\n205    M\n206    M\n207    M\nName: 60, Length: 208, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"As it has just two labels so I can do is just get dummies and then drop the first dummy. As I have two dummy columns then I can drop one.","metadata":{}},{"cell_type":"code","source":"y = pd.get_dummies(y, drop_first = True)\ny.sample(5) # R ------> 1 & M -------> 0","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:27:19.547090Z","iopub.execute_input":"2023-03-28T06:27:19.547827Z","iopub.status.idle":"2023-03-28T06:27:19.564352Z","shell.execute_reply.started":"2023-03-28T06:27:19.547788Z","shell.execute_reply":"2023-03-28T06:27:19.563044Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"     R\n83   1\n124  0\n72   1\n77   1\n36   1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Here I have R columns which means 1 --> ROck & 0--> Metal.  Here we need only one column and that will be my target","metadata":{}},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:29:53.327360Z","iopub.execute_input":"2023-03-28T06:29:53.327815Z","iopub.status.idle":"2023-03-28T06:29:53.340943Z","shell.execute_reply.started":"2023-03-28T06:29:53.327773Z","shell.execute_reply":"2023-03-28T06:29:53.339686Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"R\n0    111\n1     97\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Here we can see my Rock and Metal finally converted into 0 and 1. Now my x & y are ready.\n\nSo the next step in Machine learning is divide data into train and test dataset. I am using test size of 25 percent. I use random state for reproducibility and if we are running this again and again, if we specifiy random state it will divided into similar type of splits.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:38:15.460202Z","iopub.execute_input":"2023-03-28T06:38:15.460895Z","iopub.status.idle":"2023-03-28T06:38:15.468008Z","shell.execute_reply.started":"2023-03-28T06:38:15.460854Z","shell.execute_reply":"2023-03-28T06:38:15.466913Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:38:30.176280Z","iopub.execute_input":"2023-03-28T06:38:30.176680Z","iopub.status.idle":"2023-03-28T06:38:30.186811Z","shell.execute_reply.started":"2023-03-28T06:38:30.176644Z","shell.execute_reply":"2023-03-28T06:38:30.185471Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"((156, 60), (52, 60))"},"metadata":{}}]},{"cell_type":"code","source":"x_test","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:41:03.075377Z","iopub.execute_input":"2023-03-28T06:41:03.076643Z","iopub.status.idle":"2023-03-28T06:41:03.148287Z","shell.execute_reply.started":"2023-03-28T06:41:03.076589Z","shell.execute_reply":"2023-03-28T06:41:03.147280Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       8   \\\n186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n155  0.0211  0.0128  0.0015  0.0450  0.0711  0.1563  0.1518  0.1206  0.1666   \n165  0.0221  0.0065  0.0164  0.0487  0.0519  0.0849  0.0812  0.1833  0.2228   \n200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n58   0.0225  0.0019  0.0075  0.0097  0.0445  0.0906  0.0889  0.0655  0.1624   \n34   0.0311  0.0491  0.0692  0.0831  0.0079  0.0200  0.0981  0.1016  0.2025   \n151  0.0231  0.0315  0.0170  0.0226  0.0410  0.0116  0.0223  0.0805  0.2365   \n18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n202  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n62   0.0086  0.0215  0.0242  0.0445  0.0667  0.0771  0.0499  0.0906  0.1229   \n4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n47   0.0373  0.0281  0.0232  0.0225  0.0179  0.0733  0.0841  0.1031  0.0993   \n110  0.0210  0.0121  0.0203  0.1036  0.1675  0.0418  0.0723  0.0828  0.0494   \n206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n105  0.0116  0.0179  0.0449  0.1096  0.1913  0.0924  0.0761  0.1092  0.0757   \n172  0.0180  0.0444  0.0476  0.0698  0.1615  0.0887  0.0596  0.1071  0.3175   \n31   0.0084  0.0153  0.0291  0.0432  0.0951  0.0752  0.0414  0.0259  0.0692   \n198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n33   0.0442  0.0477  0.0049  0.0581  0.0278  0.0678  0.1664  0.1490  0.0974   \n40   0.0068  0.0232  0.0513  0.0444  0.0249  0.0637  0.0422  0.1130  0.1911   \n175  0.0294  0.0123  0.0117  0.0113  0.0497  0.0998  0.1326  0.1117  0.2984   \n59   0.0125  0.0152  0.0218  0.0175  0.0362  0.0696  0.0873  0.0616  0.1252   \n29   0.0189  0.0308  0.0197  0.0622  0.0080  0.0789  0.1440  0.1451  0.1789   \n11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n124  0.0388  0.0324  0.0688  0.0898  0.1267  0.1515  0.2134  0.2613  0.2832   \n147  0.0654  0.0649  0.0737  0.1132  0.2482  0.1257  0.1797  0.0989  0.2460   \n35   0.0206  0.0132  0.0533  0.0569  0.0647  0.1432  0.1344  0.2041  0.1571   \n44   0.0257  0.0447  0.0388  0.0239  0.1315  0.1323  0.1608  0.2145  0.0847   \n51   0.0131  0.0068  0.0308  0.0311  0.0085  0.0767  0.0771  0.0640  0.0726   \n171  0.0179  0.0136  0.0408  0.0633  0.0596  0.0808  0.2090  0.3465  0.5276   \n153  0.0233  0.0394  0.0416  0.0547  0.0993  0.1515  0.1674  0.1513  0.1723   \n183  0.0096  0.0404  0.0682  0.0688  0.0887  0.0932  0.0955  0.2140  0.2546   \n28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n16   0.0352  0.0116  0.0191  0.0469  0.0737  0.1185  0.1683  0.1541  0.1466   \n94   0.0025  0.0309  0.0171  0.0228  0.0434  0.1224  0.1947  0.1661  0.1368   \n78   0.0231  0.0351  0.0030  0.0304  0.0339  0.0860  0.1738  0.1351  0.1063   \n38   0.0123  0.0022  0.0196  0.0206  0.0180  0.0492  0.0033  0.0398  0.0791   \n27   0.0177  0.0300  0.0288  0.0394  0.0630  0.0526  0.0688  0.0633  0.0624   \n69   0.0216  0.0215  0.0273  0.0139  0.0357  0.0785  0.0906  0.0908  0.1151   \n119  0.0261  0.0266  0.0223  0.0749  0.1364  0.1513  0.1316  0.1654  0.1864   \n207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n73   0.0139  0.0222  0.0089  0.0108  0.0215  0.0136  0.0659  0.0954  0.0786   \n166  0.0411  0.0277  0.0604  0.0525  0.0489  0.0385  0.0611  0.1117  0.1237   \n138  0.0731  0.1249  0.1665  0.1496  0.1443  0.2770  0.2555  0.1712  0.0466   \n199  0.0116  0.0744  0.0367  0.0225  0.0076  0.0545  0.1110  0.1069  0.1708   \n84   0.0378  0.0318  0.0423  0.0350  0.1787  0.1635  0.0887  0.0817  0.1779   \n90   0.0126  0.0519  0.0621  0.0518  0.1072  0.2587  0.2304  0.2067  0.3416   \n123  0.0270  0.0163  0.0341  0.0247  0.0822  0.1256  0.1323  0.1584  0.2017   \n169  0.0130  0.0120  0.0436  0.0624  0.0428  0.0349  0.0384  0.0446  0.1318   \n107  0.0428  0.0555  0.0708  0.0618  0.1215  0.1524  0.1543  0.0391  0.0610   \n201  0.0335  0.0258  0.0398  0.0570  0.0529  0.1091  0.1709  0.1684  0.1865   \n\n         9   ...      50      51      52      53      54      55      56  \\\n186  0.3914  ...  0.0054  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062   \n155  0.1345  ...  0.0174  0.0117  0.0023  0.0047  0.0049  0.0031  0.0024   \n165  0.1810  ...  0.0167  0.0089  0.0051  0.0015  0.0075  0.0058  0.0016   \n200  0.3193  ...  0.0137  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040   \n58   0.1452  ...  0.0051  0.0034  0.0129  0.0100  0.0044  0.0057  0.0030   \n34   0.0767  ...  0.0089  0.0087  0.0032  0.0130  0.0188  0.0101  0.0229   \n151  0.2461  ...  0.0151  0.0125  0.0036  0.0123  0.0043  0.0114  0.0052   \n18   0.1520  ...  0.0045  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120   \n202  0.3997  ...  0.0146  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054   \n62   0.1185  ...  0.0047  0.0072  0.0054  0.0022  0.0016  0.0029  0.0058   \n4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n47   0.0802  ...  0.0066  0.0008  0.0045  0.0024  0.0006  0.0073  0.0096   \n110  0.0686  ...  0.0104  0.0117  0.0101  0.0061  0.0031  0.0099  0.0080   \n206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n105  0.1006  ...  0.0031  0.0163  0.0099  0.0084  0.0270  0.0277  0.0097   \n172  0.2918  ...  0.0122  0.0122  0.0114  0.0098  0.0027  0.0025  0.0026   \n31   0.1753  ...  0.0056  0.0236  0.0114  0.0136  0.0117  0.0060  0.0058   \n198  0.2048  ...  0.0186  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028   \n33   0.1268  ...  0.0210  0.0204  0.0216  0.0135  0.0055  0.0073  0.0080   \n40   0.2475  ...  0.0199  0.0173  0.0163  0.0055  0.0045  0.0068  0.0041   \n175  0.3473  ...  0.0041  0.0056  0.0104  0.0079  0.0014  0.0054  0.0015   \n59   0.1302  ...  0.0019  0.0041  0.0074  0.0030  0.0050  0.0048  0.0017   \n29   0.2522  ...  0.0091  0.0038  0.0096  0.0142  0.0190  0.0140  0.0099   \n11   0.0835  ...  0.0188  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026   \n124  0.2718  ...  0.0152  0.0255  0.0071  0.0263  0.0079  0.0111  0.0107   \n147  0.3422  ...  0.0243  0.0210  0.0361  0.0239  0.0447  0.0394  0.0355   \n35   0.1573  ...  0.0307  0.0386  0.0147  0.0018  0.0100  0.0096  0.0077   \n44   0.0561  ...  0.0206  0.0096  0.0153  0.0096  0.0131  0.0198  0.0025   \n51   0.0901  ...  0.0109  0.0062  0.0028  0.0040  0.0075  0.0039  0.0053   \n171  0.5965  ...  0.0086  0.0123  0.0060  0.0187  0.0111  0.0126  0.0081   \n153  0.2078  ...  0.0071  0.0104  0.0062  0.0026  0.0025  0.0061  0.0038   \n183  0.2952  ...  0.0310  0.0237  0.0078  0.0144  0.0170  0.0012  0.0109   \n28   0.2668  ...  0.0118  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117   \n16   0.2912  ...  0.0426  0.0346  0.0158  0.0154  0.0109  0.0048  0.0095   \n94   0.1430  ...  0.0108  0.0149  0.0077  0.0036  0.0114  0.0085  0.0101   \n78   0.0347  ...  0.0154  0.0106  0.0097  0.0022  0.0052  0.0072  0.0056   \n38   0.0475  ...  0.0149  0.0125  0.0134  0.0026  0.0038  0.0018  0.0113   \n27   0.0613  ...  0.0168  0.0102  0.0122  0.0044  0.0075  0.0124  0.0099   \n69   0.0973  ...  0.0082  0.0140  0.0044  0.0052  0.0073  0.0021  0.0047   \n119  0.2013  ...  0.0135  0.0222  0.0175  0.0127  0.0022  0.0124  0.0054   \n207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n191  0.1533  ...  0.0138  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054   \n73   0.1015  ...  0.0024  0.0062  0.0072  0.0113  0.0012  0.0022  0.0025   \n166  0.2300  ...  0.0181  0.0217  0.0038  0.0019  0.0065  0.0132  0.0108   \n138  0.1114  ...  0.0361  0.0444  0.0230  0.0290  0.0141  0.0161  0.0177   \n199  0.2271  ...  0.0202  0.0141  0.0103  0.0100  0.0034  0.0026  0.0037   \n84   0.2053  ...  0.0093  0.0046  0.0044  0.0078  0.0102  0.0065  0.0061   \n90   0.4284  ...  0.0027  0.0208  0.0048  0.0199  0.0126  0.0022  0.0037   \n123  0.2122  ...  0.0197  0.0189  0.0204  0.0085  0.0043  0.0092  0.0138   \n169  0.1375  ...  0.0024  0.0084  0.0100  0.0018  0.0035  0.0058  0.0011   \n107  0.0113  ...  0.0009  0.0142  0.0179  0.0079  0.0060  0.0131  0.0089   \n201  0.2660  ...  0.0130  0.0120  0.0039  0.0053  0.0062  0.0046  0.0045   \n\n         57      58      59  \n186  0.0087  0.0070  0.0042  \n155  0.0039  0.0051  0.0015  \n165  0.0070  0.0074  0.0038  \n200  0.0009  0.0015  0.0085  \n58   0.0035  0.0021  0.0027  \n34   0.0182  0.0046  0.0038  \n151  0.0091  0.0008  0.0092  \n18   0.0132  0.0070  0.0088  \n202  0.0051  0.0065  0.0103  \n62   0.0050  0.0024  0.0030  \n4    0.0048  0.0107  0.0094  \n47   0.0054  0.0085  0.0060  \n110  0.0107  0.0161  0.0133  \n206  0.0079  0.0036  0.0048  \n105  0.0054  0.0148  0.0092  \n172  0.0050  0.0073  0.0022  \n31   0.0031  0.0072  0.0045  \n198  0.0013  0.0035  0.0060  \n33   0.0105  0.0059  0.0105  \n40   0.0052  0.0194  0.0105  \n175  0.0006  0.0081  0.0043  \n59   0.0041  0.0086  0.0058  \n29   0.0092  0.0052  0.0075  \n11   0.0092  0.0009  0.0044  \n124  0.0068  0.0097  0.0067  \n147  0.0440  0.0243  0.0098  \n35   0.0180  0.0109  0.0070  \n44   0.0199  0.0255  0.0180  \n51   0.0013  0.0052  0.0023  \n171  0.0155  0.0160  0.0085  \n153  0.0101  0.0078  0.0006  \n183  0.0036  0.0043  0.0018  \n28   0.0020  0.0091  0.0058  \n16   0.0015  0.0073  0.0067  \n94   0.0016  0.0028  0.0014  \n78   0.0038  0.0043  0.0030  \n38   0.0058  0.0047  0.0071  \n27   0.0057  0.0032  0.0019  \n69   0.0024  0.0009  0.0017  \n119  0.0021  0.0028  0.0023  \n207  0.0036  0.0061  0.0115  \n191  0.0035  0.0001  0.0055  \n73   0.0059  0.0039  0.0048  \n166  0.0050  0.0085  0.0044  \n138  0.0194  0.0207  0.0057  \n199  0.0044  0.0057  0.0035  \n84   0.0062  0.0043  0.0053  \n90   0.0034  0.0114  0.0077  \n123  0.0094  0.0105  0.0093  \n169  0.0009  0.0033  0.0026  \n107  0.0084  0.0113  0.0049  \n201  0.0022  0.0005  0.0031  \n\n[52 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>186</th>\n      <td>0.0209</td>\n      <td>0.0191</td>\n      <td>0.0411</td>\n      <td>0.0321</td>\n      <td>0.0698</td>\n      <td>0.1579</td>\n      <td>0.1438</td>\n      <td>0.1402</td>\n      <td>0.3048</td>\n      <td>0.3914</td>\n      <td>...</td>\n      <td>0.0054</td>\n      <td>0.0078</td>\n      <td>0.0201</td>\n      <td>0.0104</td>\n      <td>0.0039</td>\n      <td>0.0031</td>\n      <td>0.0062</td>\n      <td>0.0087</td>\n      <td>0.0070</td>\n      <td>0.0042</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>0.0211</td>\n      <td>0.0128</td>\n      <td>0.0015</td>\n      <td>0.0450</td>\n      <td>0.0711</td>\n      <td>0.1563</td>\n      <td>0.1518</td>\n      <td>0.1206</td>\n      <td>0.1666</td>\n      <td>0.1345</td>\n      <td>...</td>\n      <td>0.0174</td>\n      <td>0.0117</td>\n      <td>0.0023</td>\n      <td>0.0047</td>\n      <td>0.0049</td>\n      <td>0.0031</td>\n      <td>0.0024</td>\n      <td>0.0039</td>\n      <td>0.0051</td>\n      <td>0.0015</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>0.0221</td>\n      <td>0.0065</td>\n      <td>0.0164</td>\n      <td>0.0487</td>\n      <td>0.0519</td>\n      <td>0.0849</td>\n      <td>0.0812</td>\n      <td>0.1833</td>\n      <td>0.2228</td>\n      <td>0.1810</td>\n      <td>...</td>\n      <td>0.0167</td>\n      <td>0.0089</td>\n      <td>0.0051</td>\n      <td>0.0015</td>\n      <td>0.0075</td>\n      <td>0.0058</td>\n      <td>0.0016</td>\n      <td>0.0070</td>\n      <td>0.0074</td>\n      <td>0.0038</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.0131</td>\n      <td>0.0387</td>\n      <td>0.0329</td>\n      <td>0.0078</td>\n      <td>0.0721</td>\n      <td>0.1341</td>\n      <td>0.1626</td>\n      <td>0.1902</td>\n      <td>0.2610</td>\n      <td>0.3193</td>\n      <td>...</td>\n      <td>0.0137</td>\n      <td>0.0150</td>\n      <td>0.0076</td>\n      <td>0.0032</td>\n      <td>0.0037</td>\n      <td>0.0071</td>\n      <td>0.0040</td>\n      <td>0.0009</td>\n      <td>0.0015</td>\n      <td>0.0085</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>0.0225</td>\n      <td>0.0019</td>\n      <td>0.0075</td>\n      <td>0.0097</td>\n      <td>0.0445</td>\n      <td>0.0906</td>\n      <td>0.0889</td>\n      <td>0.0655</td>\n      <td>0.1624</td>\n      <td>0.1452</td>\n      <td>...</td>\n      <td>0.0051</td>\n      <td>0.0034</td>\n      <td>0.0129</td>\n      <td>0.0100</td>\n      <td>0.0044</td>\n      <td>0.0057</td>\n      <td>0.0030</td>\n      <td>0.0035</td>\n      <td>0.0021</td>\n      <td>0.0027</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.0311</td>\n      <td>0.0491</td>\n      <td>0.0692</td>\n      <td>0.0831</td>\n      <td>0.0079</td>\n      <td>0.0200</td>\n      <td>0.0981</td>\n      <td>0.1016</td>\n      <td>0.2025</td>\n      <td>0.0767</td>\n      <td>...</td>\n      <td>0.0089</td>\n      <td>0.0087</td>\n      <td>0.0032</td>\n      <td>0.0130</td>\n      <td>0.0188</td>\n      <td>0.0101</td>\n      <td>0.0229</td>\n      <td>0.0182</td>\n      <td>0.0046</td>\n      <td>0.0038</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>0.0231</td>\n      <td>0.0315</td>\n      <td>0.0170</td>\n      <td>0.0226</td>\n      <td>0.0410</td>\n      <td>0.0116</td>\n      <td>0.0223</td>\n      <td>0.0805</td>\n      <td>0.2365</td>\n      <td>0.2461</td>\n      <td>...</td>\n      <td>0.0151</td>\n      <td>0.0125</td>\n      <td>0.0036</td>\n      <td>0.0123</td>\n      <td>0.0043</td>\n      <td>0.0114</td>\n      <td>0.0052</td>\n      <td>0.0091</td>\n      <td>0.0008</td>\n      <td>0.0092</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0270</td>\n      <td>0.0092</td>\n      <td>0.0145</td>\n      <td>0.0278</td>\n      <td>0.0412</td>\n      <td>0.0757</td>\n      <td>0.1026</td>\n      <td>0.1138</td>\n      <td>0.0794</td>\n      <td>0.1520</td>\n      <td>...</td>\n      <td>0.0045</td>\n      <td>0.0084</td>\n      <td>0.0010</td>\n      <td>0.0018</td>\n      <td>0.0068</td>\n      <td>0.0039</td>\n      <td>0.0120</td>\n      <td>0.0132</td>\n      <td>0.0070</td>\n      <td>0.0088</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>0.0272</td>\n      <td>0.0378</td>\n      <td>0.0488</td>\n      <td>0.0848</td>\n      <td>0.1127</td>\n      <td>0.1103</td>\n      <td>0.1349</td>\n      <td>0.2337</td>\n      <td>0.3113</td>\n      <td>0.3997</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0091</td>\n      <td>0.0045</td>\n      <td>0.0043</td>\n      <td>0.0043</td>\n      <td>0.0098</td>\n      <td>0.0054</td>\n      <td>0.0051</td>\n      <td>0.0065</td>\n      <td>0.0103</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0.0086</td>\n      <td>0.0215</td>\n      <td>0.0242</td>\n      <td>0.0445</td>\n      <td>0.0667</td>\n      <td>0.0771</td>\n      <td>0.0499</td>\n      <td>0.0906</td>\n      <td>0.1229</td>\n      <td>0.1185</td>\n      <td>...</td>\n      <td>0.0047</td>\n      <td>0.0072</td>\n      <td>0.0054</td>\n      <td>0.0022</td>\n      <td>0.0016</td>\n      <td>0.0029</td>\n      <td>0.0058</td>\n      <td>0.0050</td>\n      <td>0.0024</td>\n      <td>0.0030</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0156</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.0373</td>\n      <td>0.0281</td>\n      <td>0.0232</td>\n      <td>0.0225</td>\n      <td>0.0179</td>\n      <td>0.0733</td>\n      <td>0.0841</td>\n      <td>0.1031</td>\n      <td>0.0993</td>\n      <td>0.0802</td>\n      <td>...</td>\n      <td>0.0066</td>\n      <td>0.0008</td>\n      <td>0.0045</td>\n      <td>0.0024</td>\n      <td>0.0006</td>\n      <td>0.0073</td>\n      <td>0.0096</td>\n      <td>0.0054</td>\n      <td>0.0085</td>\n      <td>0.0060</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>0.0210</td>\n      <td>0.0121</td>\n      <td>0.0203</td>\n      <td>0.1036</td>\n      <td>0.1675</td>\n      <td>0.0418</td>\n      <td>0.0723</td>\n      <td>0.0828</td>\n      <td>0.0494</td>\n      <td>0.0686</td>\n      <td>...</td>\n      <td>0.0104</td>\n      <td>0.0117</td>\n      <td>0.0101</td>\n      <td>0.0061</td>\n      <td>0.0031</td>\n      <td>0.0099</td>\n      <td>0.0080</td>\n      <td>0.0107</td>\n      <td>0.0161</td>\n      <td>0.0133</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>0.0116</td>\n      <td>0.0179</td>\n      <td>0.0449</td>\n      <td>0.1096</td>\n      <td>0.1913</td>\n      <td>0.0924</td>\n      <td>0.0761</td>\n      <td>0.1092</td>\n      <td>0.0757</td>\n      <td>0.1006</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0163</td>\n      <td>0.0099</td>\n      <td>0.0084</td>\n      <td>0.0270</td>\n      <td>0.0277</td>\n      <td>0.0097</td>\n      <td>0.0054</td>\n      <td>0.0148</td>\n      <td>0.0092</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>0.0180</td>\n      <td>0.0444</td>\n      <td>0.0476</td>\n      <td>0.0698</td>\n      <td>0.1615</td>\n      <td>0.0887</td>\n      <td>0.0596</td>\n      <td>0.1071</td>\n      <td>0.3175</td>\n      <td>0.2918</td>\n      <td>...</td>\n      <td>0.0122</td>\n      <td>0.0122</td>\n      <td>0.0114</td>\n      <td>0.0098</td>\n      <td>0.0027</td>\n      <td>0.0025</td>\n      <td>0.0026</td>\n      <td>0.0050</td>\n      <td>0.0073</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.0084</td>\n      <td>0.0153</td>\n      <td>0.0291</td>\n      <td>0.0432</td>\n      <td>0.0951</td>\n      <td>0.0752</td>\n      <td>0.0414</td>\n      <td>0.0259</td>\n      <td>0.0692</td>\n      <td>0.1753</td>\n      <td>...</td>\n      <td>0.0056</td>\n      <td>0.0236</td>\n      <td>0.0114</td>\n      <td>0.0136</td>\n      <td>0.0117</td>\n      <td>0.0060</td>\n      <td>0.0058</td>\n      <td>0.0031</td>\n      <td>0.0072</td>\n      <td>0.0045</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>0.0238</td>\n      <td>0.0318</td>\n      <td>0.0422</td>\n      <td>0.0399</td>\n      <td>0.0788</td>\n      <td>0.0766</td>\n      <td>0.0881</td>\n      <td>0.1143</td>\n      <td>0.1594</td>\n      <td>0.2048</td>\n      <td>...</td>\n      <td>0.0186</td>\n      <td>0.0096</td>\n      <td>0.0071</td>\n      <td>0.0084</td>\n      <td>0.0038</td>\n      <td>0.0026</td>\n      <td>0.0028</td>\n      <td>0.0013</td>\n      <td>0.0035</td>\n      <td>0.0060</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.0442</td>\n      <td>0.0477</td>\n      <td>0.0049</td>\n      <td>0.0581</td>\n      <td>0.0278</td>\n      <td>0.0678</td>\n      <td>0.1664</td>\n      <td>0.1490</td>\n      <td>0.0974</td>\n      <td>0.1268</td>\n      <td>...</td>\n      <td>0.0210</td>\n      <td>0.0204</td>\n      <td>0.0216</td>\n      <td>0.0135</td>\n      <td>0.0055</td>\n      <td>0.0073</td>\n      <td>0.0080</td>\n      <td>0.0105</td>\n      <td>0.0059</td>\n      <td>0.0105</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.0068</td>\n      <td>0.0232</td>\n      <td>0.0513</td>\n      <td>0.0444</td>\n      <td>0.0249</td>\n      <td>0.0637</td>\n      <td>0.0422</td>\n      <td>0.1130</td>\n      <td>0.1911</td>\n      <td>0.2475</td>\n      <td>...</td>\n      <td>0.0199</td>\n      <td>0.0173</td>\n      <td>0.0163</td>\n      <td>0.0055</td>\n      <td>0.0045</td>\n      <td>0.0068</td>\n      <td>0.0041</td>\n      <td>0.0052</td>\n      <td>0.0194</td>\n      <td>0.0105</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>0.0294</td>\n      <td>0.0123</td>\n      <td>0.0117</td>\n      <td>0.0113</td>\n      <td>0.0497</td>\n      <td>0.0998</td>\n      <td>0.1326</td>\n      <td>0.1117</td>\n      <td>0.2984</td>\n      <td>0.3473</td>\n      <td>...</td>\n      <td>0.0041</td>\n      <td>0.0056</td>\n      <td>0.0104</td>\n      <td>0.0079</td>\n      <td>0.0014</td>\n      <td>0.0054</td>\n      <td>0.0015</td>\n      <td>0.0006</td>\n      <td>0.0081</td>\n      <td>0.0043</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>0.0125</td>\n      <td>0.0152</td>\n      <td>0.0218</td>\n      <td>0.0175</td>\n      <td>0.0362</td>\n      <td>0.0696</td>\n      <td>0.0873</td>\n      <td>0.0616</td>\n      <td>0.1252</td>\n      <td>0.1302</td>\n      <td>...</td>\n      <td>0.0019</td>\n      <td>0.0041</td>\n      <td>0.0074</td>\n      <td>0.0030</td>\n      <td>0.0050</td>\n      <td>0.0048</td>\n      <td>0.0017</td>\n      <td>0.0041</td>\n      <td>0.0086</td>\n      <td>0.0058</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.0189</td>\n      <td>0.0308</td>\n      <td>0.0197</td>\n      <td>0.0622</td>\n      <td>0.0080</td>\n      <td>0.0789</td>\n      <td>0.1440</td>\n      <td>0.1451</td>\n      <td>0.1789</td>\n      <td>0.2522</td>\n      <td>...</td>\n      <td>0.0091</td>\n      <td>0.0038</td>\n      <td>0.0096</td>\n      <td>0.0142</td>\n      <td>0.0190</td>\n      <td>0.0140</td>\n      <td>0.0099</td>\n      <td>0.0092</td>\n      <td>0.0052</td>\n      <td>0.0075</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.0123</td>\n      <td>0.0309</td>\n      <td>0.0169</td>\n      <td>0.0313</td>\n      <td>0.0358</td>\n      <td>0.0102</td>\n      <td>0.0182</td>\n      <td>0.0579</td>\n      <td>0.1122</td>\n      <td>0.0835</td>\n      <td>...</td>\n      <td>0.0188</td>\n      <td>0.0133</td>\n      <td>0.0265</td>\n      <td>0.0224</td>\n      <td>0.0074</td>\n      <td>0.0118</td>\n      <td>0.0026</td>\n      <td>0.0092</td>\n      <td>0.0009</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>0.0388</td>\n      <td>0.0324</td>\n      <td>0.0688</td>\n      <td>0.0898</td>\n      <td>0.1267</td>\n      <td>0.1515</td>\n      <td>0.2134</td>\n      <td>0.2613</td>\n      <td>0.2832</td>\n      <td>0.2718</td>\n      <td>...</td>\n      <td>0.0152</td>\n      <td>0.0255</td>\n      <td>0.0071</td>\n      <td>0.0263</td>\n      <td>0.0079</td>\n      <td>0.0111</td>\n      <td>0.0107</td>\n      <td>0.0068</td>\n      <td>0.0097</td>\n      <td>0.0067</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.0654</td>\n      <td>0.0649</td>\n      <td>0.0737</td>\n      <td>0.1132</td>\n      <td>0.2482</td>\n      <td>0.1257</td>\n      <td>0.1797</td>\n      <td>0.0989</td>\n      <td>0.2460</td>\n      <td>0.3422</td>\n      <td>...</td>\n      <td>0.0243</td>\n      <td>0.0210</td>\n      <td>0.0361</td>\n      <td>0.0239</td>\n      <td>0.0447</td>\n      <td>0.0394</td>\n      <td>0.0355</td>\n      <td>0.0440</td>\n      <td>0.0243</td>\n      <td>0.0098</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.0206</td>\n      <td>0.0132</td>\n      <td>0.0533</td>\n      <td>0.0569</td>\n      <td>0.0647</td>\n      <td>0.1432</td>\n      <td>0.1344</td>\n      <td>0.2041</td>\n      <td>0.1571</td>\n      <td>0.1573</td>\n      <td>...</td>\n      <td>0.0307</td>\n      <td>0.0386</td>\n      <td>0.0147</td>\n      <td>0.0018</td>\n      <td>0.0100</td>\n      <td>0.0096</td>\n      <td>0.0077</td>\n      <td>0.0180</td>\n      <td>0.0109</td>\n      <td>0.0070</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.0257</td>\n      <td>0.0447</td>\n      <td>0.0388</td>\n      <td>0.0239</td>\n      <td>0.1315</td>\n      <td>0.1323</td>\n      <td>0.1608</td>\n      <td>0.2145</td>\n      <td>0.0847</td>\n      <td>0.0561</td>\n      <td>...</td>\n      <td>0.0206</td>\n      <td>0.0096</td>\n      <td>0.0153</td>\n      <td>0.0096</td>\n      <td>0.0131</td>\n      <td>0.0198</td>\n      <td>0.0025</td>\n      <td>0.0199</td>\n      <td>0.0255</td>\n      <td>0.0180</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.0131</td>\n      <td>0.0068</td>\n      <td>0.0308</td>\n      <td>0.0311</td>\n      <td>0.0085</td>\n      <td>0.0767</td>\n      <td>0.0771</td>\n      <td>0.0640</td>\n      <td>0.0726</td>\n      <td>0.0901</td>\n      <td>...</td>\n      <td>0.0109</td>\n      <td>0.0062</td>\n      <td>0.0028</td>\n      <td>0.0040</td>\n      <td>0.0075</td>\n      <td>0.0039</td>\n      <td>0.0053</td>\n      <td>0.0013</td>\n      <td>0.0052</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>0.0179</td>\n      <td>0.0136</td>\n      <td>0.0408</td>\n      <td>0.0633</td>\n      <td>0.0596</td>\n      <td>0.0808</td>\n      <td>0.2090</td>\n      <td>0.3465</td>\n      <td>0.5276</td>\n      <td>0.5965</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0123</td>\n      <td>0.0060</td>\n      <td>0.0187</td>\n      <td>0.0111</td>\n      <td>0.0126</td>\n      <td>0.0081</td>\n      <td>0.0155</td>\n      <td>0.0160</td>\n      <td>0.0085</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>0.0233</td>\n      <td>0.0394</td>\n      <td>0.0416</td>\n      <td>0.0547</td>\n      <td>0.0993</td>\n      <td>0.1515</td>\n      <td>0.1674</td>\n      <td>0.1513</td>\n      <td>0.1723</td>\n      <td>0.2078</td>\n      <td>...</td>\n      <td>0.0071</td>\n      <td>0.0104</td>\n      <td>0.0062</td>\n      <td>0.0026</td>\n      <td>0.0025</td>\n      <td>0.0061</td>\n      <td>0.0038</td>\n      <td>0.0101</td>\n      <td>0.0078</td>\n      <td>0.0006</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.0096</td>\n      <td>0.0404</td>\n      <td>0.0682</td>\n      <td>0.0688</td>\n      <td>0.0887</td>\n      <td>0.0932</td>\n      <td>0.0955</td>\n      <td>0.2140</td>\n      <td>0.2546</td>\n      <td>0.2952</td>\n      <td>...</td>\n      <td>0.0310</td>\n      <td>0.0237</td>\n      <td>0.0078</td>\n      <td>0.0144</td>\n      <td>0.0170</td>\n      <td>0.0012</td>\n      <td>0.0109</td>\n      <td>0.0036</td>\n      <td>0.0043</td>\n      <td>0.0018</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.0100</td>\n      <td>0.0275</td>\n      <td>0.0190</td>\n      <td>0.0371</td>\n      <td>0.0416</td>\n      <td>0.0201</td>\n      <td>0.0314</td>\n      <td>0.0651</td>\n      <td>0.1896</td>\n      <td>0.2668</td>\n      <td>...</td>\n      <td>0.0118</td>\n      <td>0.0088</td>\n      <td>0.0104</td>\n      <td>0.0036</td>\n      <td>0.0088</td>\n      <td>0.0047</td>\n      <td>0.0117</td>\n      <td>0.0020</td>\n      <td>0.0091</td>\n      <td>0.0058</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.0352</td>\n      <td>0.0116</td>\n      <td>0.0191</td>\n      <td>0.0469</td>\n      <td>0.0737</td>\n      <td>0.1185</td>\n      <td>0.1683</td>\n      <td>0.1541</td>\n      <td>0.1466</td>\n      <td>0.2912</td>\n      <td>...</td>\n      <td>0.0426</td>\n      <td>0.0346</td>\n      <td>0.0158</td>\n      <td>0.0154</td>\n      <td>0.0109</td>\n      <td>0.0048</td>\n      <td>0.0095</td>\n      <td>0.0015</td>\n      <td>0.0073</td>\n      <td>0.0067</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>0.0025</td>\n      <td>0.0309</td>\n      <td>0.0171</td>\n      <td>0.0228</td>\n      <td>0.0434</td>\n      <td>0.1224</td>\n      <td>0.1947</td>\n      <td>0.1661</td>\n      <td>0.1368</td>\n      <td>0.1430</td>\n      <td>...</td>\n      <td>0.0108</td>\n      <td>0.0149</td>\n      <td>0.0077</td>\n      <td>0.0036</td>\n      <td>0.0114</td>\n      <td>0.0085</td>\n      <td>0.0101</td>\n      <td>0.0016</td>\n      <td>0.0028</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>0.0231</td>\n      <td>0.0351</td>\n      <td>0.0030</td>\n      <td>0.0304</td>\n      <td>0.0339</td>\n      <td>0.0860</td>\n      <td>0.1738</td>\n      <td>0.1351</td>\n      <td>0.1063</td>\n      <td>0.0347</td>\n      <td>...</td>\n      <td>0.0154</td>\n      <td>0.0106</td>\n      <td>0.0097</td>\n      <td>0.0022</td>\n      <td>0.0052</td>\n      <td>0.0072</td>\n      <td>0.0056</td>\n      <td>0.0038</td>\n      <td>0.0043</td>\n      <td>0.0030</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.0123</td>\n      <td>0.0022</td>\n      <td>0.0196</td>\n      <td>0.0206</td>\n      <td>0.0180</td>\n      <td>0.0492</td>\n      <td>0.0033</td>\n      <td>0.0398</td>\n      <td>0.0791</td>\n      <td>0.0475</td>\n      <td>...</td>\n      <td>0.0149</td>\n      <td>0.0125</td>\n      <td>0.0134</td>\n      <td>0.0026</td>\n      <td>0.0038</td>\n      <td>0.0018</td>\n      <td>0.0113</td>\n      <td>0.0058</td>\n      <td>0.0047</td>\n      <td>0.0071</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.0177</td>\n      <td>0.0300</td>\n      <td>0.0288</td>\n      <td>0.0394</td>\n      <td>0.0630</td>\n      <td>0.0526</td>\n      <td>0.0688</td>\n      <td>0.0633</td>\n      <td>0.0624</td>\n      <td>0.0613</td>\n      <td>...</td>\n      <td>0.0168</td>\n      <td>0.0102</td>\n      <td>0.0122</td>\n      <td>0.0044</td>\n      <td>0.0075</td>\n      <td>0.0124</td>\n      <td>0.0099</td>\n      <td>0.0057</td>\n      <td>0.0032</td>\n      <td>0.0019</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>0.0216</td>\n      <td>0.0215</td>\n      <td>0.0273</td>\n      <td>0.0139</td>\n      <td>0.0357</td>\n      <td>0.0785</td>\n      <td>0.0906</td>\n      <td>0.0908</td>\n      <td>0.1151</td>\n      <td>0.0973</td>\n      <td>...</td>\n      <td>0.0082</td>\n      <td>0.0140</td>\n      <td>0.0044</td>\n      <td>0.0052</td>\n      <td>0.0073</td>\n      <td>0.0021</td>\n      <td>0.0047</td>\n      <td>0.0024</td>\n      <td>0.0009</td>\n      <td>0.0017</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>0.0261</td>\n      <td>0.0266</td>\n      <td>0.0223</td>\n      <td>0.0749</td>\n      <td>0.1364</td>\n      <td>0.1513</td>\n      <td>0.1316</td>\n      <td>0.1654</td>\n      <td>0.1864</td>\n      <td>0.2013</td>\n      <td>...</td>\n      <td>0.0135</td>\n      <td>0.0222</td>\n      <td>0.0175</td>\n      <td>0.0127</td>\n      <td>0.0022</td>\n      <td>0.0124</td>\n      <td>0.0054</td>\n      <td>0.0021</td>\n      <td>0.0028</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0181</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>0.0315</td>\n      <td>0.0252</td>\n      <td>0.0167</td>\n      <td>0.0479</td>\n      <td>0.0902</td>\n      <td>0.1057</td>\n      <td>0.1024</td>\n      <td>0.1209</td>\n      <td>0.1241</td>\n      <td>0.1533</td>\n      <td>...</td>\n      <td>0.0138</td>\n      <td>0.0108</td>\n      <td>0.0062</td>\n      <td>0.0044</td>\n      <td>0.0072</td>\n      <td>0.0007</td>\n      <td>0.0054</td>\n      <td>0.0035</td>\n      <td>0.0001</td>\n      <td>0.0055</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>0.0139</td>\n      <td>0.0222</td>\n      <td>0.0089</td>\n      <td>0.0108</td>\n      <td>0.0215</td>\n      <td>0.0136</td>\n      <td>0.0659</td>\n      <td>0.0954</td>\n      <td>0.0786</td>\n      <td>0.1015</td>\n      <td>...</td>\n      <td>0.0024</td>\n      <td>0.0062</td>\n      <td>0.0072</td>\n      <td>0.0113</td>\n      <td>0.0012</td>\n      <td>0.0022</td>\n      <td>0.0025</td>\n      <td>0.0059</td>\n      <td>0.0039</td>\n      <td>0.0048</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>0.0411</td>\n      <td>0.0277</td>\n      <td>0.0604</td>\n      <td>0.0525</td>\n      <td>0.0489</td>\n      <td>0.0385</td>\n      <td>0.0611</td>\n      <td>0.1117</td>\n      <td>0.1237</td>\n      <td>0.2300</td>\n      <td>...</td>\n      <td>0.0181</td>\n      <td>0.0217</td>\n      <td>0.0038</td>\n      <td>0.0019</td>\n      <td>0.0065</td>\n      <td>0.0132</td>\n      <td>0.0108</td>\n      <td>0.0050</td>\n      <td>0.0085</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>0.0731</td>\n      <td>0.1249</td>\n      <td>0.1665</td>\n      <td>0.1496</td>\n      <td>0.1443</td>\n      <td>0.2770</td>\n      <td>0.2555</td>\n      <td>0.1712</td>\n      <td>0.0466</td>\n      <td>0.1114</td>\n      <td>...</td>\n      <td>0.0361</td>\n      <td>0.0444</td>\n      <td>0.0230</td>\n      <td>0.0290</td>\n      <td>0.0141</td>\n      <td>0.0161</td>\n      <td>0.0177</td>\n      <td>0.0194</td>\n      <td>0.0207</td>\n      <td>0.0057</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.0116</td>\n      <td>0.0744</td>\n      <td>0.0367</td>\n      <td>0.0225</td>\n      <td>0.0076</td>\n      <td>0.0545</td>\n      <td>0.1110</td>\n      <td>0.1069</td>\n      <td>0.1708</td>\n      <td>0.2271</td>\n      <td>...</td>\n      <td>0.0202</td>\n      <td>0.0141</td>\n      <td>0.0103</td>\n      <td>0.0100</td>\n      <td>0.0034</td>\n      <td>0.0026</td>\n      <td>0.0037</td>\n      <td>0.0044</td>\n      <td>0.0057</td>\n      <td>0.0035</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>0.0378</td>\n      <td>0.0318</td>\n      <td>0.0423</td>\n      <td>0.0350</td>\n      <td>0.1787</td>\n      <td>0.1635</td>\n      <td>0.0887</td>\n      <td>0.0817</td>\n      <td>0.1779</td>\n      <td>0.2053</td>\n      <td>...</td>\n      <td>0.0093</td>\n      <td>0.0046</td>\n      <td>0.0044</td>\n      <td>0.0078</td>\n      <td>0.0102</td>\n      <td>0.0065</td>\n      <td>0.0061</td>\n      <td>0.0062</td>\n      <td>0.0043</td>\n      <td>0.0053</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>0.0126</td>\n      <td>0.0519</td>\n      <td>0.0621</td>\n      <td>0.0518</td>\n      <td>0.1072</td>\n      <td>0.2587</td>\n      <td>0.2304</td>\n      <td>0.2067</td>\n      <td>0.3416</td>\n      <td>0.4284</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0208</td>\n      <td>0.0048</td>\n      <td>0.0199</td>\n      <td>0.0126</td>\n      <td>0.0022</td>\n      <td>0.0037</td>\n      <td>0.0034</td>\n      <td>0.0114</td>\n      <td>0.0077</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>0.0270</td>\n      <td>0.0163</td>\n      <td>0.0341</td>\n      <td>0.0247</td>\n      <td>0.0822</td>\n      <td>0.1256</td>\n      <td>0.1323</td>\n      <td>0.1584</td>\n      <td>0.2017</td>\n      <td>0.2122</td>\n      <td>...</td>\n      <td>0.0197</td>\n      <td>0.0189</td>\n      <td>0.0204</td>\n      <td>0.0085</td>\n      <td>0.0043</td>\n      <td>0.0092</td>\n      <td>0.0138</td>\n      <td>0.0094</td>\n      <td>0.0105</td>\n      <td>0.0093</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>0.0130</td>\n      <td>0.0120</td>\n      <td>0.0436</td>\n      <td>0.0624</td>\n      <td>0.0428</td>\n      <td>0.0349</td>\n      <td>0.0384</td>\n      <td>0.0446</td>\n      <td>0.1318</td>\n      <td>0.1375</td>\n      <td>...</td>\n      <td>0.0024</td>\n      <td>0.0084</td>\n      <td>0.0100</td>\n      <td>0.0018</td>\n      <td>0.0035</td>\n      <td>0.0058</td>\n      <td>0.0011</td>\n      <td>0.0009</td>\n      <td>0.0033</td>\n      <td>0.0026</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>0.0428</td>\n      <td>0.0555</td>\n      <td>0.0708</td>\n      <td>0.0618</td>\n      <td>0.1215</td>\n      <td>0.1524</td>\n      <td>0.1543</td>\n      <td>0.0391</td>\n      <td>0.0610</td>\n      <td>0.0113</td>\n      <td>...</td>\n      <td>0.0009</td>\n      <td>0.0142</td>\n      <td>0.0179</td>\n      <td>0.0079</td>\n      <td>0.0060</td>\n      <td>0.0131</td>\n      <td>0.0089</td>\n      <td>0.0084</td>\n      <td>0.0113</td>\n      <td>0.0049</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>0.0335</td>\n      <td>0.0258</td>\n      <td>0.0398</td>\n      <td>0.0570</td>\n      <td>0.0529</td>\n      <td>0.1091</td>\n      <td>0.1709</td>\n      <td>0.1684</td>\n      <td>0.1865</td>\n      <td>0.2660</td>\n      <td>...</td>\n      <td>0.0130</td>\n      <td>0.0120</td>\n      <td>0.0039</td>\n      <td>0.0053</td>\n      <td>0.0062</td>\n      <td>0.0046</td>\n      <td>0.0045</td>\n      <td>0.0022</td>\n      <td>0.0005</td>\n      <td>0.0031</td>\n    </tr>\n  </tbody>\n</table>\n<p>52 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Using Deep Learning Model","metadata":{}},{"cell_type":"markdown","source":"### Model without dropout layer","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Dense(60, input_dim = 60, activation = 'relu'), #as we have 60 neurons in input layer(for 60 columns in total) and they are connecting to hidden layer which also have 60 neuron\n     #will create 2 more hidden layers\n    keras.layers.Dense(30, activation = 'relu'),\n    keras.layers.Dense(15, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid') #this is a output layer where I have 1 neuron because of binary classification problem\n])\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics =['accuracy'])\nmodel.fit(x_train, y_train, epochs=100, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:46:57.603189Z","iopub.execute_input":"2023-03-28T06:46:57.603691Z","iopub.status.idle":"2023-03-28T06:47:08.666875Z","shell.execute_reply.started":"2023-03-28T06:46:57.603649Z","shell.execute_reply":"2023-03-28T06:47:08.665770Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/100\n20/20 [==============================] - 1s 4ms/step - loss: 0.6866 - accuracy: 0.5897\nEpoch 2/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6653 - accuracy: 0.6474\nEpoch 3/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.6795\nEpoch 4/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.7179\nEpoch 5/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.7051\nEpoch 6/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.7179\nEpoch 7/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.5270 - accuracy: 0.7308\nEpoch 8/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4857 - accuracy: 0.7949\nEpoch 9/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4532 - accuracy: 0.8205\nEpoch 10/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8205\nEpoch 11/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8462\nEpoch 12/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3663 - accuracy: 0.8718\nEpoch 13/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3590 - accuracy: 0.8333\nEpoch 14/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3394 - accuracy: 0.8654\nEpoch 15/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.8654\nEpoch 16/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3706 - accuracy: 0.8013\nEpoch 17/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8462\nEpoch 18/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8462\nEpoch 19/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.8910\nEpoch 20/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.9167\nEpoch 21/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.9038\nEpoch 22/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.9231\nEpoch 23/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9167\nEpoch 24/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.9103\nEpoch 25/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9103\nEpoch 26/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9167\nEpoch 27/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9103\nEpoch 28/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2199 - accuracy: 0.9038\nEpoch 29/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9038\nEpoch 30/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9551\nEpoch 31/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9551\nEpoch 32/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9295\nEpoch 33/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.9487\nEpoch 34/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1461 - accuracy: 0.9615\nEpoch 35/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9615\nEpoch 36/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9615\nEpoch 37/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9423\nEpoch 38/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9615\nEpoch 39/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9679\nEpoch 40/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.9615\nEpoch 41/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9679\nEpoch 42/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9744\nEpoch 43/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.9679\nEpoch 44/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9808\nEpoch 45/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.9679\nEpoch 46/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9936\nEpoch 47/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9936\nEpoch 48/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9744\nEpoch 49/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9936\nEpoch 50/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 1.0000\nEpoch 51/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 1.0000\nEpoch 52/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9936\nEpoch 53/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 1.0000\nEpoch 54/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 1.0000\nEpoch 55/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 1.0000\nEpoch 56/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 1.0000\nEpoch 57/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 1.0000\nEpoch 58/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 1.0000\nEpoch 59/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 1.0000\nEpoch 60/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 1.0000\nEpoch 61/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 1.0000\nEpoch 62/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9872\nEpoch 63/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9936\nEpoch 64/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 1.0000\nEpoch 65/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 1.0000\nEpoch 66/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 1.0000\nEpoch 67/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 1.0000\nEpoch 68/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\nEpoch 69/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 1.0000\nEpoch 70/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 1.0000\nEpoch 71/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 1.0000\nEpoch 72/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 1.0000\nEpoch 73/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\nEpoch 74/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 1.0000\nEpoch 75/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 1.0000\nEpoch 76/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 1.0000\nEpoch 77/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\nEpoch 78/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 1.0000\nEpoch 79/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 1.0000\nEpoch 80/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\nEpoch 81/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000\nEpoch 82/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 1.0000\nEpoch 83/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 1.0000\nEpoch 84/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 1.0000\nEpoch 85/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000\nEpoch 86/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000\nEpoch 87/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000\nEpoch 88/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 1.0000\nEpoch 89/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\nEpoch 90/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 1.0000\nEpoch 91/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\nEpoch 92/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000\nEpoch 93/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000\nEpoch 94/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 1.0000\nEpoch 95/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000\nEpoch 96/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.0026 - accuracy: 1.0000\nEpoch 97/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000\nEpoch 98/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.0024 - accuracy: 1.0000\nEpoch 99/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\nEpoch 100/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f84980f72d0>"},"metadata":{}}]},{"cell_type":"code","source":"model.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:47:46.741768Z","iopub.execute_input":"2023-03-28T06:47:46.742131Z","iopub.status.idle":"2023-03-28T06:47:46.996348Z","shell.execute_reply.started":"2023-03-28T06:47:46.742100Z","shell.execute_reply":"2023-03-28T06:47:46.995373Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 0s 6ms/step - loss: 1.1544 - accuracy: 0.7308\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"[1.1543891429901123, 0.7307692170143127]"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = model.predict(x_test).reshape(-1)\nprint(y_pred[:10])\n\n#Sigmoid predicts 0 & 1 so I need to round the values to nearest integer ie 0 or 1\ny_pred = np.round(y_pred)\nprint(y_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:49:49.970893Z","iopub.execute_input":"2023-03-28T06:49:49.971578Z","iopub.status.idle":"2023-03-28T06:49:50.201273Z","shell.execute_reply.started":"2023-03-28T06:49:49.971531Z","shell.execute_reply":"2023-03-28T06:49:50.200238Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 0s 4ms/step\n[4.7849943e-07 9.0081114e-01 8.4761119e-01 1.2867355e-05 9.9999976e-01\n 9.9995506e-01 2.4774756e-02 9.9999833e-01 1.7322011e-05 9.9999988e-01]\n[0. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test[:10]","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:50:05.958642Z","iopub.execute_input":"2023-03-28T06:50:05.959021Z","iopub.status.idle":"2023-03-28T06:50:05.968771Z","shell.execute_reply.started":"2023-03-28T06:50:05.958983Z","shell.execute_reply":"2023-03-28T06:50:05.967475Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"     R\n186  0\n155  0\n165  0\n200  0\n58   1\n34   1\n151  0\n18   1\n202  0\n62   1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>186</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:50:20.141962Z","iopub.execute_input":"2023-03-28T06:50:20.142348Z","iopub.status.idle":"2023-03-28T06:50:20.157647Z","shell.execute_reply.started":"2023-03-28T06:50:20.142299Z","shell.execute_reply":"2023-03-28T06:50:20.156406Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.70      0.85      0.77        27\n           1       0.79      0.60      0.68        25\n\n    accuracy                           0.73        52\n   macro avg       0.74      0.73      0.72        52\nweighted avg       0.74      0.73      0.73        52\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model with Dropout Layer","metadata":{}},{"cell_type":"code","source":"modeld = keras.Sequential([\n    keras.layers.Dense(60, input_dim=60, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(30, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(15, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid') #this is a output layer where I have 1 neuron because of binary classification problem\n])\n\nmodeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodeld.fit(x_train, y_train, epochs=100, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:51:21.484256Z","iopub.execute_input":"2023-03-28T06:51:21.485279Z","iopub.status.idle":"2023-03-28T06:51:30.985474Z","shell.execute_reply.started":"2023-03-28T06:51:21.485237Z","shell.execute_reply":"2023-03-28T06:51:30.984405Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1/100\n20/20 [==============================] - 2s 4ms/step - loss: 0.7487 - accuracy: 0.5256\nEpoch 2/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.5641\nEpoch 3/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5192\nEpoch 4/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.7054 - accuracy: 0.5577\nEpoch 5/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.7141 - accuracy: 0.5385\nEpoch 6/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5513\nEpoch 7/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.7012 - accuracy: 0.5513\nEpoch 8/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5321\nEpoch 9/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.6154\nEpoch 10/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.7050 - accuracy: 0.5192\nEpoch 11/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5192\nEpoch 12/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5513\nEpoch 13/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5000\nEpoch 14/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.5449\nEpoch 15/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6733 - accuracy: 0.5192\nEpoch 16/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5385\nEpoch 17/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6788 - accuracy: 0.5513\nEpoch 18/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.5769\nEpoch 19/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.5962\nEpoch 20/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.5705\nEpoch 21/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6589 - accuracy: 0.6410\nEpoch 22/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6090\nEpoch 23/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6781 - accuracy: 0.5897\nEpoch 24/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5833\nEpoch 25/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 0.6026\nEpoch 26/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6538\nEpoch 27/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.6090\nEpoch 28/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.6026\nEpoch 29/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6544 - accuracy: 0.5962\nEpoch 30/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.6026\nEpoch 31/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6096 - accuracy: 0.6603\nEpoch 32/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.6474\nEpoch 33/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6024 - accuracy: 0.6795\nEpoch 34/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6080 - accuracy: 0.6731\nEpoch 35/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.6253 - accuracy: 0.6987\nEpoch 36/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5860 - accuracy: 0.6667\nEpoch 37/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6859\nEpoch 38/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.6028 - accuracy: 0.6603\nEpoch 39/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.7244\nEpoch 40/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5559 - accuracy: 0.7115\nEpoch 41/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5433 - accuracy: 0.7628\nEpoch 42/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5405 - accuracy: 0.7115\nEpoch 43/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5764 - accuracy: 0.7308\nEpoch 44/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.6603\nEpoch 45/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.6859\nEpoch 46/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5611 - accuracy: 0.7051\nEpoch 47/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5594 - accuracy: 0.7179\nEpoch 48/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7692\nEpoch 49/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7821\nEpoch 50/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.5098 - accuracy: 0.7756\nEpoch 51/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.7500\nEpoch 52/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.5872 - accuracy: 0.6859\nEpoch 53/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4475 - accuracy: 0.8269\nEpoch 54/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.7692\nEpoch 55/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4833 - accuracy: 0.7564\nEpoch 56/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4963 - accuracy: 0.7628\nEpoch 57/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4708 - accuracy: 0.7564\nEpoch 58/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7692\nEpoch 59/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4677 - accuracy: 0.8013\nEpoch 60/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7564\nEpoch 61/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.8013\nEpoch 62/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.7628\nEpoch 63/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8397\nEpoch 64/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4007 - accuracy: 0.8333\nEpoch 65/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.7949\nEpoch 66/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8333\nEpoch 67/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8462\nEpoch 68/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.7885\nEpoch 69/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8397\nEpoch 70/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4394 - accuracy: 0.7949\nEpoch 71/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4069 - accuracy: 0.8269\nEpoch 72/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.7949\nEpoch 73/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3795 - accuracy: 0.8526\nEpoch 74/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.7885\nEpoch 75/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8333\nEpoch 76/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8462\nEpoch 77/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8397\nEpoch 78/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.8077\nEpoch 79/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8718\nEpoch 80/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8462\nEpoch 81/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3931 - accuracy: 0.8269\nEpoch 82/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.7885\nEpoch 83/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3648 - accuracy: 0.8590\nEpoch 84/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8846\nEpoch 85/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.2951 - accuracy: 0.8846\nEpoch 86/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8718\nEpoch 87/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8526\nEpoch 88/100\n20/20 [==============================] - 0s 3ms/step - loss: 0.4176 - accuracy: 0.8397\nEpoch 89/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.8590\nEpoch 90/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3532 - accuracy: 0.8590\nEpoch 91/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3498 - accuracy: 0.8590\nEpoch 92/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.8910\nEpoch 93/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.8718\nEpoch 94/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3042 - accuracy: 0.8782\nEpoch 95/100\n20/20 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8654\nEpoch 96/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3039 - accuracy: 0.8846\nEpoch 97/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.2926 - accuracy: 0.9103\nEpoch 98/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8654\nEpoch 99/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3561 - accuracy: 0.8462\nEpoch 100/100\n20/20 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8782\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f848c61f550>"},"metadata":{}}]},{"cell_type":"code","source":"modeld.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:51:38.035312Z","iopub.execute_input":"2023-03-28T06:51:38.036675Z","iopub.status.idle":"2023-03-28T06:51:38.273984Z","shell.execute_reply.started":"2023-03-28T06:51:38.036621Z","shell.execute_reply":"2023-03-28T06:51:38.272799Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 0s 8ms/step - loss: 0.4505 - accuracy: 0.7692\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[0.45047956705093384, 0.7692307829856873]"},"metadata":{}}]},{"cell_type":"markdown","source":"Training Accuracy is still good but Test Accuracy Improved","metadata":{}},{"cell_type":"code","source":"y_pred = modeld.predict(x_test).reshape(-1)\nprint(y_pred[:10])\n\n# round the values to nearest integer ie 0 or 1\ny_pred = np.round(y_pred)\nprint(y_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:51:56.455836Z","iopub.execute_input":"2023-03-28T06:51:56.456209Z","iopub.status.idle":"2023-03-28T06:51:56.607143Z","shell.execute_reply.started":"2023-03-28T06:51:56.456174Z","shell.execute_reply":"2023-03-28T06:51:56.606014Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 0s 4ms/step\n[1.6750941e-04 8.1129688e-01 9.2020518e-01 2.4325792e-02 9.9400228e-01\n 9.4838077e-01 3.3766431e-01 9.8964334e-01 1.5428182e-02 9.9607015e-01]\n[0. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:52:05.360124Z","iopub.execute_input":"2023-03-28T06:52:05.361184Z","iopub.status.idle":"2023-03-28T06:52:05.373858Z","shell.execute_reply.started":"2023-03-28T06:52:05.361143Z","shell.execute_reply":"2023-03-28T06:52:05.372394Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.76      0.81      0.79        27\n           1       0.78      0.72      0.75        25\n\n    accuracy                           0.77        52\n   macro avg       0.77      0.77      0.77        52\nweighted avg       0.77      0.77      0.77        52\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here accuracy is increased 0.73 to 0.77 after using  dropout technique.","metadata":{}}]}